{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [Word Tokenization](#word-tokenization)\n",
    "    * [Most Common Words](#most-common-words)\n",
    "    * [Feature Creation](#most-common-words-feature-creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tweets dataset, this is a cut down version of the full tweet \n",
    "# dataset per the operations performed in the file \"eda-rea-v-liv-2018\"\n",
    "en_tweets_df = pd.read_csv('en_tweets_df.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this dataset is generated from earlier work it carries its prior index, the below changes the column name\n",
    "en_tweets_df.rename(columns={'Unnamed: 0':'original_df_index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Swear Words <a class=\"anchor\" id=\"word-tokenization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "swear_words_file = open('swear_words.txt', 'r')\n",
    "swear_words = [line.split(',') for line in swear_words_file.readlines()]\n",
    "swear_words = [j for sub in swear_words for j in sub] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean version of tweet solely for outputs, actual model will use all words\n",
    "# rough approach as it has no impact on analysis. For example \"massive\" would become \"m*profanity*ssive\"\n",
    "def CleanTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    for word in swear_words:\n",
    "        tweet = tweet.replace(word, '*profanity*')\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tweets_df['clean_tweet'] = en_tweets_df.apply(lambda row: CleanTweet(row['tweet_text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization <a class=\"anchor\" id=\"word-tokenization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that returns tokenizes, cleans and stems words for a tweet\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Context specific stop words (refer \"most common words\" section below for identification approach\n",
    "# rt = short for retweet, this does not provide any insights and a column already exists to identify retweets\n",
    "# http & https = the start of web links these provide little value as \"words\", future work: these could be\n",
    "# to build a feature along thelines of \"Contains Web Link?\"\n",
    "# uclfinal, championsleague, championsleaguefinal = \"hashtag\"/topical words, given the original tweet dataset\n",
    "# contained only tweets that had a hashtag of uclfinal these words do not add value to the analysis\n",
    "custom_stopwords = ['rt', 'http', 'https', 'uclfinal', 'championsleague', 'championsleaguefinal', 'lfcrma',  \n",
    "                    'liverpoolvsrealmadrid', 'livrma', 'realiv', 'realliverpool', 'realmadridliverpool', 'realmadridvsliverpool', \n",
    "                    'rmalfc', 'rmaliv', 'rmavlfc', 'rmavliv', 'rmavsliv', 'rmliv', 'rmvsliv', 'retweet', 'retweeted']\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Combine the two stop words lists\n",
    "stop_words = english_stopwords + custom_stopwords\n",
    "\n",
    "def TokenizeTweet(tweet):\n",
    "    word_tokenized = word_tokenize(tweet)\n",
    "\n",
    "    \n",
    "    cleaned_words_tokenized = [word.lower().strip() for word in word_tokenized] # lowercasing\n",
    "    \n",
    "    cleaned_words_tokenized = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', word) for word in cleaned_words_tokenized] # remove URLs\n",
    "    cleaned_words_tokenized = [re.sub('@[^\\s]+', 'AT_USER', word) for word in cleaned_words_tokenized] # remove usernames\n",
    "    cleaned_words_tokenized = [re.sub(r'#([^\\s]+)', r'\\1', word) for word in cleaned_words_tokenized] # remove the # in #hashtag   \n",
    "    cleaned_words_tokenized = [word.replace('(','').replace(')','') for word in cleaned_words_tokenized if word.isalpha()]  # replacing some unwanted things\n",
    "    \n",
    "    # Ideally all checks could be moved into this single for loop, rather than iterating words multiple times.\n",
    "    for n, i in enumerate(cleaned_words_tokenized):\n",
    "        if i in ['liverpool', 'lfc', 'ufcliverpool', 'liv']:\n",
    "            cleaned_words_tokenized[n] = 'liverpoolfc'\n",
    "            \n",
    "        if i in ['rma', 'madrid']:\n",
    "            cleaned_words_tokenized[n] = 'realmadrid'\n",
    "    \n",
    "    cleaned_words_tokenized = [word for word in cleaned_words_tokenized if word not in stop_words] # removing stopwords\n",
    "    cleaned_words_tokenized = [ps.stem(word) for word in cleaned_words_tokenized] # stemming\n",
    "    \n",
    "    return cleaned_words_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words in tweets\n",
    "en_tweets_df['tokenized_words'] = en_tweets_df.apply(lambda row: TokenizeTweet(row['tweet_text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Words <a class=\"anchor\" id=\"most-common-words\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are three key benefits to finding the most common words:\n",
    "    # 1. Further refinements could be made to TokenizedWords in terms of words to exclude\n",
    "    # 2. We can obtain further insights into the data\n",
    "    # 3. Can select key words that could be used to generate features    \n",
    "\n",
    "# Convert tokenized words column into a single list of words\n",
    "words_list = en_tweets_df['tokenized_words'].values.tolist()\n",
    "\n",
    "# Flatten the list\n",
    "flattened_words_list = [j for sub in words_list for j in sub]\n",
    "\n",
    "# Find the most common words\n",
    "fdist = FreqDist(x.lower() for x in flattened_words_list)\n",
    "fdist.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tweets_df_text_only = en_tweets_df[['id','tweet_text', 'tokenized_words']]\n",
    "en_tweets_df_text_only.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans <a class=\"anchor\" id=\"kmeans\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Loop  <a class=\"anchor\" id=\"tf-idf-loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_tweets_df_no_rt = en_tweets_df[en_tweets_df['is_retweet']==False]\n",
    "# en_tweets_df_no_rt.shape\n",
    "df = en_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized words column into a single list of words\n",
    "words_list = df['tokenized_words'].values.tolist()\n",
    "\n",
    "# Flatten the list\n",
    "flattened_words_list = set([j for sub in words_list for j in sub])\n",
    "\n",
    "# Get the number of distinct words in dataset\n",
    "number_of_distinct_words = len(flattened_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df = 0 ---- max_features = 25675 ---- n_clusters = 3\n",
      "min_df = 0 ---- max_features = 25675 ---- n_clusters = 6\n",
      "min_df = 0 ---- max_features = 25675 ---- n_clusters = 9\n",
      "min_df = 0 ---- max_features = 25675 ---- n_clusters = 12\n",
      "min_df = 0 ---- max_features = 25675 ---- n_clusters = 15\n",
      "min_df = 0 ---- max_features = 19256 ---- n_clusters = 3\n",
      "min_df = 0 ---- max_features = 19256 ---- n_clusters = 6\n",
      "min_df = 0 ---- max_features = 19256 ---- n_clusters = 9\n",
      "min_df = 0 ---- max_features = 19256 ---- n_clusters = 12\n",
      "min_df = 0 ---- max_features = 19256 ---- n_clusters = 15\n",
      "min_df = 0 ---- max_features = 12837 ---- n_clusters = 3\n",
      "min_df = 0 ---- max_features = 12837 ---- n_clusters = 6\n",
      "min_df = 0 ---- max_features = 12837 ---- n_clusters = 9\n",
      "min_df = 0 ---- max_features = 12837 ---- n_clusters = 12\n",
      "min_df = 0 ---- max_features = 12837 ---- n_clusters = 15\n",
      "min_df = 10 ---- max_features = 25675 ---- n_clusters = 3\n",
      "min_df = 10 ---- max_features = 25675 ---- n_clusters = 6\n",
      "min_df = 10 ---- max_features = 25675 ---- n_clusters = 9\n",
      "min_df = 10 ---- max_features = 25675 ---- n_clusters = 12\n",
      "min_df = 10 ---- max_features = 25675 ---- n_clusters = 15\n",
      "min_df = 10 ---- max_features = 19256 ---- n_clusters = 3\n",
      "min_df = 10 ---- max_features = 19256 ---- n_clusters = 6\n",
      "min_df = 10 ---- max_features = 19256 ---- n_clusters = 9\n",
      "min_df = 10 ---- max_features = 19256 ---- n_clusters = 12\n",
      "min_df = 10 ---- max_features = 19256 ---- n_clusters = 15\n",
      "min_df = 10 ---- max_features = 12837 ---- n_clusters = 3\n",
      "min_df = 10 ---- max_features = 12837 ---- n_clusters = 6\n",
      "min_df = 10 ---- max_features = 12837 ---- n_clusters = 9\n",
      "min_df = 10 ---- max_features = 12837 ---- n_clusters = 12\n",
      "min_df = 10 ---- max_features = 12837 ---- n_clusters = 15\n",
      "min_df = 20 ---- max_features = 25675 ---- n_clusters = 3\n",
      "min_df = 20 ---- max_features = 25675 ---- n_clusters = 6\n",
      "min_df = 20 ---- max_features = 25675 ---- n_clusters = 9\n",
      "min_df = 20 ---- max_features = 25675 ---- n_clusters = 12\n",
      "min_df = 20 ---- max_features = 25675 ---- n_clusters = 15\n",
      "min_df = 20 ---- max_features = 19256 ---- n_clusters = 3\n",
      "min_df = 20 ---- max_features = 19256 ---- n_clusters = 6\n",
      "min_df = 20 ---- max_features = 19256 ---- n_clusters = 9\n",
      "min_df = 20 ---- max_features = 19256 ---- n_clusters = 12\n",
      "min_df = 20 ---- max_features = 19256 ---- n_clusters = 15\n",
      "min_df = 20 ---- max_features = 12837 ---- n_clusters = 3\n",
      "min_df = 20 ---- max_features = 12837 ---- n_clusters = 6\n",
      "min_df = 20 ---- max_features = 12837 ---- n_clusters = 9\n",
      "min_df = 20 ---- max_features = 12837 ---- n_clusters = 12\n",
      "min_df = 20 ---- max_features = 12837 ---- n_clusters = 15\n",
      "min_df = 30 ---- max_features = 25675 ---- n_clusters = 3\n",
      "min_df = 30 ---- max_features = 25675 ---- n_clusters = 6\n",
      "min_df = 30 ---- max_features = 25675 ---- n_clusters = 9\n",
      "min_df = 30 ---- max_features = 25675 ---- n_clusters = 12\n",
      "min_df = 30 ---- max_features = 25675 ---- n_clusters = 15\n",
      "min_df = 30 ---- max_features = 19256 ---- n_clusters = 3\n",
      "min_df = 30 ---- max_features = 19256 ---- n_clusters = 6\n",
      "min_df = 30 ---- max_features = 19256 ---- n_clusters = 9\n",
      "min_df = 30 ---- max_features = 19256 ---- n_clusters = 12\n",
      "min_df = 30 ---- max_features = 19256 ---- n_clusters = 15\n",
      "min_df = 30 ---- max_features = 12837 ---- n_clusters = 3\n",
      "min_df = 30 ---- max_features = 12837 ---- n_clusters = 6\n",
      "min_df = 30 ---- max_features = 12837 ---- n_clusters = 9\n",
      "min_df = 30 ---- max_features = 12837 ---- n_clusters = 12\n",
      "min_df = 30 ---- max_features = 12837 ---- n_clusters = 15\n"
     ]
    }
   ],
   "source": [
    "# Loop through the feature creation changing the:\n",
    "    # minimum df - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "    # max_features - If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "    # k - Number of clusters\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "min_df = range(0, 40, 10) # Loop four times increasing the \n",
    "\n",
    "max_features_split = round(number_of_distinct_words / 4) # Divide by 4 giving us 3 groups to iterate through, decreasing the features used each iteration\n",
    "max_features = range(number_of_distinct_words, max_features_split, -max_features_split)\n",
    "\n",
    "K = range(3, 16, 3) # Number of clusters loop from 3-15 increments of 3\n",
    "\n",
    "\n",
    "df['flatten_tokenized_words'] = df.apply(lambda row: ' '.join(row['tokenized_words']), axis=1)\n",
    "\n",
    "filename = f'{datetime.datetime.now()}-COUNT-VEC.txt' #'-TF-IDF.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    #########################\n",
    "    # FEATURE CREATION\n",
    "    #########################\n",
    "    for i in min_df:\n",
    "        for j in max_features:\n",
    "            v = CountVectorizer(min_df=i, max_features=j) #TfidfVectorizer(min_df=i, max_features=j)\n",
    "            X = v.fit_transform(df['flatten_tokenized_words'])\n",
    "\n",
    "            #########################\n",
    "            # FIT AND PREDICT MODEL\n",
    "            #########################\n",
    "            for k in K:\n",
    "                kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "                kmeans.fit(X)\n",
    "                y_kmeans = kmeans.predict(X)\n",
    "\n",
    "                #########################\n",
    "                # PRINT SAMPLES\n",
    "                #########################\n",
    "                y = pd.Series(y_kmeans)\n",
    "                df['y'] = pd.Series(y)\n",
    "\n",
    "                pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "                f.write(f'------------------------------------------------------------------------\\n')\n",
    "                f.write(f'------------------------------------------------------------------------\\n')\n",
    "                f.write(f'min_df = {i} ---- max_features = {j} ---- n_clusters = {k}\\n')\n",
    "\n",
    "                for l in range(0, k):\n",
    "                    f.write('******************************************\\n')\n",
    "                    f.write(f'Cluster {l}\\n')\n",
    "                    f.write('******************************************\\n')\n",
    "                    \n",
    "                    try:\n",
    "                      f.write(df['clean_tweet'][(df['y'] == l)].sample(10).to_string())\n",
    "                    except:\n",
    "                      f.write(df['clean_tweet'][(df['y'] == l)].head().to_string())\n",
    "\n",
    "                f.write(f'\\n------------------------------------------------------------------------\\n')\n",
    "                f.write(f'------------------------------------------------------------------------\\n')\n",
    "                print(f'min_df = {i} ---- max_features = {j} ---- n_clusters = {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df = 0 ---- max_features = 25675 ---- n_clusters = 3\n",
      "min_df = 0 ---- max_features = 25675 ---- n_clusters = 6\n",
      "min_df = 0 ---- max_features = 25675 ---- n_clusters = 9\n",
      "min_df = 0 ---- max_features = 25675 ---- n_clusters = 12\n",
      "min_df = 0 ---- max_features = 25675 ---- n_clusters = 15\n",
      "min_df = 0 ---- max_features = 19256 ---- n_clusters = 3\n",
      "min_df = 0 ---- max_features = 19256 ---- n_clusters = 6\n",
      "min_df = 0 ---- max_features = 19256 ---- n_clusters = 9\n",
      "min_df = 0 ---- max_features = 19256 ---- n_clusters = 12\n",
      "min_df = 0 ---- max_features = 19256 ---- n_clusters = 15\n",
      "min_df = 0 ---- max_features = 12837 ---- n_clusters = 3\n",
      "min_df = 0 ---- max_features = 12837 ---- n_clusters = 6\n",
      "min_df = 0 ---- max_features = 12837 ---- n_clusters = 9\n",
      "min_df = 0 ---- max_features = 12837 ---- n_clusters = 12\n",
      "min_df = 0 ---- max_features = 12837 ---- n_clusters = 15\n",
      "min_df = 10 ---- max_features = 25675 ---- n_clusters = 3\n",
      "min_df = 10 ---- max_features = 25675 ---- n_clusters = 6\n",
      "min_df = 10 ---- max_features = 25675 ---- n_clusters = 9\n",
      "min_df = 10 ---- max_features = 25675 ---- n_clusters = 12\n",
      "min_df = 10 ---- max_features = 25675 ---- n_clusters = 15\n",
      "min_df = 10 ---- max_features = 19256 ---- n_clusters = 3\n",
      "min_df = 10 ---- max_features = 19256 ---- n_clusters = 6\n",
      "min_df = 10 ---- max_features = 19256 ---- n_clusters = 9\n",
      "min_df = 10 ---- max_features = 19256 ---- n_clusters = 12\n",
      "min_df = 10 ---- max_features = 19256 ---- n_clusters = 15\n",
      "min_df = 10 ---- max_features = 12837 ---- n_clusters = 3\n",
      "min_df = 10 ---- max_features = 12837 ---- n_clusters = 6\n",
      "min_df = 10 ---- max_features = 12837 ---- n_clusters = 9\n",
      "min_df = 10 ---- max_features = 12837 ---- n_clusters = 12\n",
      "min_df = 10 ---- max_features = 12837 ---- n_clusters = 15\n",
      "min_df = 20 ---- max_features = 25675 ---- n_clusters = 3\n",
      "min_df = 20 ---- max_features = 25675 ---- n_clusters = 6\n",
      "min_df = 20 ---- max_features = 25675 ---- n_clusters = 9\n",
      "min_df = 20 ---- max_features = 25675 ---- n_clusters = 12\n",
      "min_df = 20 ---- max_features = 25675 ---- n_clusters = 15\n",
      "min_df = 20 ---- max_features = 19256 ---- n_clusters = 3\n",
      "min_df = 20 ---- max_features = 19256 ---- n_clusters = 6\n",
      "min_df = 20 ---- max_features = 19256 ---- n_clusters = 9\n",
      "min_df = 20 ---- max_features = 19256 ---- n_clusters = 12\n",
      "min_df = 20 ---- max_features = 19256 ---- n_clusters = 15\n",
      "min_df = 20 ---- max_features = 12837 ---- n_clusters = 3\n",
      "min_df = 20 ---- max_features = 12837 ---- n_clusters = 6\n",
      "min_df = 20 ---- max_features = 12837 ---- n_clusters = 9\n",
      "min_df = 20 ---- max_features = 12837 ---- n_clusters = 12\n",
      "min_df = 20 ---- max_features = 12837 ---- n_clusters = 15\n",
      "min_df = 30 ---- max_features = 25675 ---- n_clusters = 3\n",
      "min_df = 30 ---- max_features = 25675 ---- n_clusters = 6\n",
      "min_df = 30 ---- max_features = 25675 ---- n_clusters = 9\n",
      "min_df = 30 ---- max_features = 25675 ---- n_clusters = 12\n",
      "min_df = 30 ---- max_features = 25675 ---- n_clusters = 15\n",
      "min_df = 30 ---- max_features = 19256 ---- n_clusters = 3\n",
      "min_df = 30 ---- max_features = 19256 ---- n_clusters = 6\n",
      "min_df = 30 ---- max_features = 19256 ---- n_clusters = 9\n",
      "min_df = 30 ---- max_features = 19256 ---- n_clusters = 12\n",
      "min_df = 30 ---- max_features = 19256 ---- n_clusters = 15\n",
      "min_df = 30 ---- max_features = 12837 ---- n_clusters = 3\n",
      "min_df = 30 ---- max_features = 12837 ---- n_clusters = 6\n",
      "min_df = 30 ---- max_features = 12837 ---- n_clusters = 9\n",
      "min_df = 30 ---- max_features = 12837 ---- n_clusters = 12\n",
      "min_df = 30 ---- max_features = 12837 ---- n_clusters = 15\n"
     ]
    }
   ],
   "source": [
    "# Loop through the feature creation changing the:\n",
    "    # minimum df - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "    # max_features - If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "    # k - Number of clusters\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "min_df = range(0, 40, 10) # Loop four times increasing the \n",
    "\n",
    "max_features_split = round(number_of_distinct_words / 4) # Divide by 4 giving us 3 groups to iterate through, decreasing the features used each iteration\n",
    "max_features = range(number_of_distinct_words, max_features_split, -max_features_split)\n",
    "\n",
    "K = range(3, 16, 3) # Number of clusters loop from 3-15 increments of 3\n",
    "\n",
    "\n",
    "df['flatten_tokenized_words'] = df.apply(lambda row: ' '.join(row['tokenized_words']), axis=1)\n",
    "\n",
    "filename = f'{datetime.datetime.now()}-TF-IDF.txt' #'-TF-IDF.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    #########################\n",
    "    # FEATURE CREATION\n",
    "    #########################\n",
    "    for i in min_df:\n",
    "        for j in max_features:\n",
    "            v = TfidfVectorizer(min_df=i, max_features=j) # CountVectorizer(min_df=i, max_features=j)\n",
    "            X = v.fit_transform(df['flatten_tokenized_words'])\n",
    "\n",
    "            #########################\n",
    "            # FIT AND PREDICT MODEL\n",
    "            #########################\n",
    "            for k in K:\n",
    "                kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "                kmeans.fit(X)\n",
    "                y_kmeans = kmeans.predict(X)\n",
    "\n",
    "                #########################\n",
    "                # PRINT SAMPLES\n",
    "                #########################\n",
    "                y = pd.Series(y_kmeans)\n",
    "                df['y'] = pd.Series(y)\n",
    "\n",
    "                pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "                f.write(f'------------------------------------------------------------------------\\n')\n",
    "                f.write(f'------------------------------------------------------------------------\\n')\n",
    "                f.write(f'min_df = {i} ---- max_features = {j} ---- n_clusters = {k}\\n')\n",
    "\n",
    "                for l in range(0, k):\n",
    "                    f.write('******************************************\\n')\n",
    "                    f.write(f'Cluster {l}\\n')\n",
    "                    f.write('******************************************\\n')\n",
    "                    \n",
    "                    try:\n",
    "                      f.write(df['clean_tweet'][(df['y'] == l)].sample(10).to_string())\n",
    "                    except:\n",
    "                      f.write(df['clean_tweet'][(df['y'] == l)].head().to_string())\n",
    "\n",
    "                f.write(f'\\n------------------------------------------------------------------------\\n')\n",
    "                f.write(f'------------------------------------------------------------------------\\n')\n",
    "                print(f'min_df = {i} ---- max_features = {j} ---- n_clusters = {k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Useful snippets etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Memory usage https://stackoverflow.com/questions/16261240/releasing-memory-of-huge-numpy-array-in-ipython/16278056\n",
    "# import sys\n",
    "# def sizeof_fmt(num, suffix='B'):\n",
    "#     ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "#         if abs(num) < 1024.0:\n",
    "#             return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "#         num /= 1024.0\n",
    "#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "#                          key= lambda x: -x[1])[:10]:\n",
    "#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a number of tweets for each cluster\n",
    "# y = pd.Series(y_kmeans)\n",
    "# df_tweet_y = pd.DataFrame(en_tweets_df)\n",
    "# df_tweet_y['y'] = pd.Series(y)\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# print(f'------------------------------------------------------------------------')\n",
    "# print(f'------------------------------------------------------------------------')\n",
    "# # print(f'k = {k} ---- Number of FT cols = {num_of_FT_cols}')\n",
    "# print(f'Data shape = {X.shape}')\n",
    "\n",
    "# for i in range(0, k):\n",
    "#     print(f'****************************************** \\n Cluster {i}')\n",
    "#     print('******************************************')\n",
    "#     print(df_tweet_y['tweet_text'][(df_tweet_y['y'] == i)].sample(15).to_string())\n",
    "#     print('****************************************** \\n')\n",
    "\n",
    "# print(f'------------------------------------------------------------------------')\n",
    "# print(f'------------------------------------------------------------------------\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return 3 - 10 clusters for cummulative sets of 25 columns, the first 25 are the most common words\n",
    "# for k in range(3, 11):\n",
    "#     for num_of_FT_cols in range(25, 501, 25):\n",
    "#         # Create X ensuring only records that have at least one of the features are included\n",
    "#         X = en_tweets_df_with_features[cols[:num_of_FT_cols]]\n",
    "#         X['sum_of_FT_cols'] = X.sum(axis=1)\n",
    "#         X = X[cols[:num_of_FT_cols]][X['sum_of_FT_cols'] != 0]\n",
    "\n",
    "#         # Fit and predict model\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "#         kmeans.fit(X)\n",
    "#         y_kmeans = kmeans.predict(X)\n",
    "        \n",
    "#         # Print a number of tweets for each cluster\n",
    "#         y = pd.Series(y_kmeans)\n",
    "#         df_tweet_y = pd.DataFrame(en_tweets_df_with_features)\n",
    "#         df_tweet_y['y'] = pd.Series(y)\n",
    "\n",
    "#         pd.set_option('display.max_colwidth', None)\n",
    "        \n",
    "#         print(f'------------------------------------------------------------------------')\n",
    "#         print(f'------------------------------------------------------------------------')\n",
    "#         print(f'k = {k} ---- Number of FT cols = {num_of_FT_cols}')\n",
    "#         print(f'Data shape = {X.shape}')\n",
    "\n",
    "#         for i in range(0, k):\n",
    "#             print(f'****************************************** \\n Cluster {i}')\n",
    "#             print('******************************************')\n",
    "#             print(df_tweet_y['tweet_text'][(df_tweet_y['y'] == i) & (df_tweet_y['is_retweet'] == False)].sample(5).to_string())\n",
    "#             print('****************************************** \\n')\n",
    "            \n",
    "#         print(f'------------------------------------------------------------------------')\n",
    "#         print(f'------------------------------------------------------------------------\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the above diagrams 7 clusters with 225 looks like it could hold some insights\n",
    "# Create X ensuring only records that have at least one of the features are included\n",
    "# k = 6\n",
    "# X = en_tweets_df[cols]\n",
    "# # X['sum_of_FT_cols'] = X.sum(axis=1)\n",
    "# # X = X[cols][X['sum_of_FT_cols'] != 0]\n",
    "\n",
    "# # Fit and predict model\n",
    "# kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "# kmeans.fit(X)\n",
    "# y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify the optimal number of clusters (k), also considering the number of feature columns. This should assist in review of the cluster samples output.\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# K = range(3, 11)\n",
    "# for num_of_FT_cols in range(25, 251, 25):\n",
    "#     distortions = []\n",
    "#     for k in K:\n",
    "#         # Create X ensuring only records that have at least one of the features are included\n",
    "#         X = en_tweets_df_with_features[cols[:num_of_FT_cols]]\n",
    "#         X['sum_of_FT_cols'] = X.sum(axis=1)\n",
    "#         X = X[cols[:num_of_FT_cols]][X['sum_of_FT_cols'] != 0]\n",
    "\n",
    "#         X_scaled = preprocessing.scale(X)\n",
    "\n",
    "#         # Fit and predict model\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "#         kmeans.fit(X_scaled)\n",
    "#         distortions.append(kmeans.inertia_)\n",
    "        \n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(K, distortions, 'bx-')\n",
    "#     plt.xlabel('k')\n",
    "#     plt.ylabel('Distortion')\n",
    "#     plt.title(f'The Elbow Method showing the optimal k with {num_of_FT_cols} feature columns')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run PCA on the data and reduce the dimensions in pca_num_components dimensions\n",
    "# reduced_data = PCA(n_components=2).fit_transform(X)\n",
    "# results = pd.DataFrame(reduced_data,columns=['pca1','pca2'])\n",
    "\n",
    "# plt.figure(figsize=(40,30))\n",
    "# sns.scatterplot(x=\"pca1\", y=\"pca2\", hue=y_kmeans, data=results)\n",
    "# plt.title('K-means Clustering')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# distortions = []\n",
    "\n",
    "# K = range(3, 11)\n",
    "# for k in K:\n",
    "#     X = x\n",
    "\n",
    "#     # Fit and predict model\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "#     kmeans.fit(X)\n",
    "#     distortions.append(kmeans.inertia_)\n",
    "        \n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(K, distortions, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Distortion')\n",
    "# plt.title(f'The Elbow Method showing the optimal k with')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
