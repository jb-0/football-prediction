{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [Word Tokenization](#word-tokenization)\n",
    "    * [Most Common Words](#most-common-words)\n",
    "    * [Feature Creation](#most-common-words-feature-creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tweets dataset, this is a cut down version of the full tweet \n",
    "# dataset per the operations performed in the file \"eda-rea-v-liv-2018\"\n",
    "en_tweets_df = pd.read_csv('en_tweets_df.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this dataset is generated from earlier work it carries its prior index, the below changes the column name\n",
    "en_tweets_df.rename(columns={'Unnamed: 0':'original_df_index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization <a class=\"anchor\" id=\"word-tokenization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that returns tokenizes, cleans and stems words for a tweet\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Context specific stop words (refer \"most common words\" section below for identification approach\n",
    "# rt = short for retweet, this does not provide any insights and a column already exists to identify retweets\n",
    "# http & https = the start of web links these provide little value as \"words\", future work: these could be\n",
    "# to build a feature along thelines of \"Contains Web Link?\"\n",
    "# uclfinal, championsleague, championsleaguefinal = \"hashtag\"/topical words, given the original tweet dataset\n",
    "# contained only tweets that had a hashtag of uclfinal these words do not add value to the analysis\n",
    "custom_stopwords = ['rt', 'http', 'https', 'uclfinal', 'championsleague', 'championsleaguefinal', 'lfcrma',  \n",
    "                    'liverpoolvsrealmadrid', 'livrma', 'realiv', 'realliverpool', 'realmadridliverpool', 'realmadridvsliverpool', \n",
    "                    'rmalfc', 'rmaliv', 'rmavlfc', 'rmavliv', 'rmavsliv', 'rmliv', 'rmvsliv', 'retweet', 'retweeted']\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Combine the two stop words lists\n",
    "stop_words = english_stopwords + custom_stopwords\n",
    "\n",
    "def TokenizeTweet(tweet):\n",
    "    word_tokenized = word_tokenize(tweet)\n",
    "\n",
    "    \n",
    "    cleaned_words_tokenized = [word.lower().strip() for word in word_tokenized] # lowercasing\n",
    "    \n",
    "    cleaned_words_tokenized = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', word) for word in cleaned_words_tokenized] # remove URLs\n",
    "    cleaned_words_tokenized = [re.sub('@[^\\s]+', 'AT_USER', word) for word in cleaned_words_tokenized] # remove usernames\n",
    "    cleaned_words_tokenized = [re.sub(r'#([^\\s]+)', r'\\1', word) for word in cleaned_words_tokenized] # remove the # in #hashtag   \n",
    "    cleaned_words_tokenized = [word.replace('(','').replace(')','') for word in cleaned_words_tokenized if word.isalpha()]  # replacing some unwanted things\n",
    "    \n",
    "    # Ideally all checks could be moved into this single for loop, rather than iterating words multiple times.\n",
    "    for n, i in enumerate(cleaned_words_tokenized):\n",
    "        if i in ['liverpool', 'lfc', 'ufcliverpool', 'liv']:\n",
    "            cleaned_words_tokenized[n] = 'liverpoolfc'\n",
    "            \n",
    "        if i in ['rma', 'madrid']:\n",
    "            cleaned_words_tokenized[n] = 'realmadrid'\n",
    "    \n",
    "    cleaned_words_tokenized = [word for word in cleaned_words_tokenized if word not in stop_words] # removing stopwords\n",
    "    cleaned_words_tokenized = [ps.stem(word) for word in cleaned_words_tokenized] # stemming\n",
    "    \n",
    "    return cleaned_words_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words in tweets\n",
    "en_tweets_df['tokenized_words'] = en_tweets_df.apply(lambda row: TokenizeTweet(row['tweet_text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Words <a class=\"anchor\" id=\"most-common-words\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('liverpoolfc', 48196),\n",
       " ('salah', 37548),\n",
       " ('ramo', 31736),\n",
       " ('realmadrid', 28476),\n",
       " ('bale', 22325),\n",
       " ('goal', 21128),\n",
       " ('real', 20624),\n",
       " ('sergio', 14425),\n",
       " ('gareth', 11275),\n",
       " ('final', 10888),\n",
       " ('world', 8579),\n",
       " ('mo', 8537),\n",
       " ('kariu', 8418),\n",
       " ('one', 7910),\n",
       " ('ronaldo', 7447),\n",
       " ('cup', 7347),\n",
       " ('time', 7302),\n",
       " ('shoulder', 7209),\n",
       " ('game', 7119),\n",
       " ('like', 7022),\n",
       " ('win', 6609),\n",
       " ('score', 6590),\n",
       " ('ever', 6378),\n",
       " ('watch', 6345),\n",
       " ('come', 6275)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are three key benefits to finding the most common words:\n",
    "    # 1. Further refinements could be made to TokenizedWords in terms of words to exclude\n",
    "    # 2. We can obtain further insights into the data\n",
    "    # 3. Can select key words that could be used to generate features    \n",
    "\n",
    "# Convert tokenized words column into a single list of words\n",
    "words_list = en_tweets_df['tokenized_words'].values.tolist()\n",
    "\n",
    "# Flatten the list\n",
    "flattened_words_list = [j for sub in words_list for j in sub]\n",
    "\n",
    "# Find the most common words\n",
    "fdist = FreqDist(x.lower() for x in flattened_words_list)\n",
    "fdist.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tokenized_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160696</th>\n",
       "      <td>1.000474e+18</td>\n",
       "      <td>RT @mufcgif: WHAT. A. GOAL. #UCLfinal https://...</td>\n",
       "      <td>[mufcgif, goal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33360</th>\n",
       "      <td>1.000455e+18</td>\n",
       "      <td>Ramos is a cunt pass it on #UCLfinal</td>\n",
       "      <td>[ramo, cunt, pass]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101918</th>\n",
       "      <td>1.000465e+18</td>\n",
       "      <td>RT @KingAbsolute_: Ronaldo thinking: He doesn'...</td>\n",
       "      <td>[ronaldo, think, know, plan, poor, salah]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138384</th>\n",
       "      <td>1.000471e+18</td>\n",
       "      <td>RT @adriandelmonte: BBC reporting Salah will m...</td>\n",
       "      <td>[adriandelmont, bbc, report, salah, miss, worl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74583</th>\n",
       "      <td>1.000461e+18</td>\n",
       "      <td>All i see is fakeness here 🤧🤧🤧🤧\\n#UCL https://...</td>\n",
       "      <td>[see, fake, ucl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                         tweet_text  \\\n",
       "160696  1.000474e+18  RT @mufcgif: WHAT. A. GOAL. #UCLfinal https://...   \n",
       "33360   1.000455e+18               Ramos is a cunt pass it on #UCLfinal   \n",
       "101918  1.000465e+18  RT @KingAbsolute_: Ronaldo thinking: He doesn'...   \n",
       "138384  1.000471e+18  RT @adriandelmonte: BBC reporting Salah will m...   \n",
       "74583   1.000461e+18  All i see is fakeness here 🤧🤧🤧🤧\\n#UCL https://...   \n",
       "\n",
       "                                          tokenized_words  \n",
       "160696                                    [mufcgif, goal]  \n",
       "33360                                  [ramo, cunt, pass]  \n",
       "101918          [ronaldo, think, know, plan, poor, salah]  \n",
       "138384  [adriandelmont, bbc, report, salah, miss, worl...  \n",
       "74583                                    [see, fake, ucl]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tweets_df_text_only = en_tweets_df[['id','tweet_text', 'tokenized_words']]\n",
    "en_tweets_df_text_only.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Feature Creation  <a class=\"anchor\" id=\"most-common-words-feature-creation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tweets_df['flatten_tokenized_words'] = en_tweets_df.apply(lambda row: ' '.join(row['tokenized_words']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer(min_df=25) # max_features=15000\n",
    "x = v.fit_transform(en_tweets_df['flatten_tokenized_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2655"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models <a class=\"anchor\" id=\"most-common-words-models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans <a class=\"anchor\" id=\"most-common-words-kmeans\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x\n",
    "\n",
    "# Fit and predict model\n",
    "kmeans = KMeans(n_clusters=6, random_state=8)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "distortions = []\n",
    "\n",
    "K = range(3, 11)\n",
    "for k in K:\n",
    "    X = x\n",
    "\n",
    "    # Fit and predict model\n",
    "    kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "    kmeans.fit(X)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "        \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title(f'The Elbow Method showing the optimal k with')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a number of tweets for each cluster\n",
    "y = pd.Series(y_kmeans)\n",
    "df_tweet_y = pd.DataFrame(en_tweets_df[en_tweets_df['is_retweet']==False])\n",
    "df_tweet_y['y'] = pd.Series(y)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(f'------------------------------------------------------------------------')\n",
    "print(f'------------------------------------------------------------------------')\n",
    "# print(f'k = {k} ---- Number of FT cols = {num_of_FT_cols}')\n",
    "print(f'Data shape = {X.shape}')\n",
    "\n",
    "for i in range(0, 6):\n",
    "    print(f'****************************************** \\n Cluster {i}')\n",
    "    print('******************************************')\n",
    "    print(df_tweet_y['tweet_text'][(df_tweet_y['y'] == i)].sample(15).to_string())\n",
    "    print('****************************************** \\n')\n",
    "\n",
    "print(f'------------------------------------------------------------------------')\n",
    "print(f'------------------------------------------------------------------------\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Loop  <a class=\"anchor\" id=\"tf-idf-loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tweets_df_no_rt = en_tweets_df[en_tweets_df['is_retweet']==True]\n",
    "\n",
    "# Convert tokenized words column into a single list of words\n",
    "words_list = en_tweets_df_no_rt['tokenized_words'].values.tolist()\n",
    "\n",
    "# Flatten the list\n",
    "flattened_words_list = set([j for sub in words_list for j in sub])\n",
    "\n",
    "# Get the number of distinct words in dataset\n",
    "number_of_distinct_words = len(flattened_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df = 0 ---- max_features = 12320 ---- n_clusters = 4\n",
      "min_df = 0 ---- max_features = 12320 ---- n_clusters = 8\n",
      "min_df = 0 ---- max_features = 12320 ---- n_clusters = 12\n",
      "min_df = 0 ---- max_features = 12320 ---- n_clusters = 16\n",
      "min_df = 0 ---- max_features = 12320 ---- n_clusters = 20\n",
      "min_df = 0 ---- max_features = 9240 ---- n_clusters = 4\n",
      "min_df = 0 ---- max_features = 9240 ---- n_clusters = 8\n",
      "min_df = 0 ---- max_features = 9240 ---- n_clusters = 12\n",
      "min_df = 0 ---- max_features = 9240 ---- n_clusters = 16\n",
      "min_df = 0 ---- max_features = 9240 ---- n_clusters = 20\n",
      "min_df = 0 ---- max_features = 6160 ---- n_clusters = 4\n",
      "min_df = 0 ---- max_features = 6160 ---- n_clusters = 8\n",
      "min_df = 0 ---- max_features = 6160 ---- n_clusters = 12\n",
      "min_df = 0 ---- max_features = 6160 ---- n_clusters = 16\n",
      "min_df = 0 ---- max_features = 6160 ---- n_clusters = 20\n",
      "min_df = 10 ---- max_features = 12320 ---- n_clusters = 4\n",
      "min_df = 10 ---- max_features = 12320 ---- n_clusters = 8\n",
      "min_df = 10 ---- max_features = 12320 ---- n_clusters = 12\n",
      "min_df = 10 ---- max_features = 12320 ---- n_clusters = 16\n",
      "min_df = 10 ---- max_features = 12320 ---- n_clusters = 20\n",
      "min_df = 10 ---- max_features = 9240 ---- n_clusters = 4\n",
      "min_df = 10 ---- max_features = 9240 ---- n_clusters = 8\n",
      "min_df = 10 ---- max_features = 9240 ---- n_clusters = 12\n",
      "min_df = 10 ---- max_features = 9240 ---- n_clusters = 16\n",
      "min_df = 10 ---- max_features = 9240 ---- n_clusters = 20\n",
      "min_df = 10 ---- max_features = 6160 ---- n_clusters = 4\n",
      "min_df = 10 ---- max_features = 6160 ---- n_clusters = 8\n",
      "min_df = 10 ---- max_features = 6160 ---- n_clusters = 12\n",
      "min_df = 10 ---- max_features = 6160 ---- n_clusters = 16\n",
      "min_df = 10 ---- max_features = 6160 ---- n_clusters = 20\n",
      "min_df = 20 ---- max_features = 12320 ---- n_clusters = 4\n",
      "min_df = 20 ---- max_features = 12320 ---- n_clusters = 8\n",
      "min_df = 20 ---- max_features = 12320 ---- n_clusters = 12\n",
      "min_df = 20 ---- max_features = 12320 ---- n_clusters = 16\n",
      "min_df = 20 ---- max_features = 12320 ---- n_clusters = 20\n",
      "min_df = 20 ---- max_features = 9240 ---- n_clusters = 4\n",
      "min_df = 20 ---- max_features = 9240 ---- n_clusters = 8\n",
      "min_df = 20 ---- max_features = 9240 ---- n_clusters = 12\n",
      "min_df = 20 ---- max_features = 9240 ---- n_clusters = 16\n",
      "min_df = 20 ---- max_features = 9240 ---- n_clusters = 20\n",
      "min_df = 20 ---- max_features = 6160 ---- n_clusters = 4\n",
      "min_df = 20 ---- max_features = 6160 ---- n_clusters = 8\n",
      "min_df = 20 ---- max_features = 6160 ---- n_clusters = 12\n",
      "min_df = 20 ---- max_features = 6160 ---- n_clusters = 16\n",
      "min_df = 20 ---- max_features = 6160 ---- n_clusters = 20\n",
      "min_df = 30 ---- max_features = 12320 ---- n_clusters = 4\n",
      "min_df = 30 ---- max_features = 12320 ---- n_clusters = 8\n",
      "min_df = 30 ---- max_features = 12320 ---- n_clusters = 12\n",
      "min_df = 30 ---- max_features = 12320 ---- n_clusters = 16\n",
      "min_df = 30 ---- max_features = 12320 ---- n_clusters = 20\n",
      "min_df = 30 ---- max_features = 9240 ---- n_clusters = 4\n",
      "min_df = 30 ---- max_features = 9240 ---- n_clusters = 8\n",
      "min_df = 30 ---- max_features = 9240 ---- n_clusters = 12\n",
      "min_df = 30 ---- max_features = 9240 ---- n_clusters = 16\n",
      "min_df = 30 ---- max_features = 9240 ---- n_clusters = 20\n",
      "min_df = 30 ---- max_features = 6160 ---- n_clusters = 4\n",
      "min_df = 30 ---- max_features = 6160 ---- n_clusters = 8\n",
      "min_df = 30 ---- max_features = 6160 ---- n_clusters = 12\n",
      "min_df = 30 ---- max_features = 6160 ---- n_clusters = 16\n",
      "min_df = 30 ---- max_features = 6160 ---- n_clusters = 20\n"
     ]
    }
   ],
   "source": [
    "# Loop through the feature creation changing the:\n",
    "    # minimum df - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "    # max_features - If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "    # k - Number of clusters\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "min_df = range(0, 40, 10) # Loop four times increasing the \n",
    "\n",
    "max_features_split = round(number_of_distinct_words / 4) # Divide by 4 giving us 3 groups to iterate through, decreasing the features used each iteration\n",
    "max_features = range(number_of_distinct_words, max_features_split, -max_features_split)\n",
    "\n",
    "K = range(4, 21, 4) # Number of clusters loop from 4-20 increments of 4\n",
    "\n",
    "df = en_tweets_df_no_rt\n",
    "\n",
    "filename = f'{datetime.datetime.now()}-TF-IDF.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    #########################\n",
    "    # FEATURE CREATION\n",
    "    #########################\n",
    "    for i in min_df:\n",
    "        for j in max_features:\n",
    "            v = TfidfVectorizer(min_df=i, max_features=j)\n",
    "            X = v.fit_transform(df['flatten_tokenized_words'])\n",
    "\n",
    "            #########################\n",
    "            # FIT AND PREDICT MODEL\n",
    "            #########################\n",
    "            for k in K:\n",
    "                kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "                kmeans.fit(X)\n",
    "                y_kmeans = kmeans.predict(X)\n",
    "\n",
    "                #########################\n",
    "                # PRINT SAMPLES\n",
    "                #########################\n",
    "                y = pd.Series(y_kmeans)\n",
    "                df['y'] = pd.Series(y)\n",
    "\n",
    "                pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "                f.write(f'------------------------------------------------------------------------\\n')\n",
    "                f.write(f'------------------------------------------------------------------------\\n')\n",
    "                f.write(f'min_df = {i} ---- max_features = {j} ---- n_clusters = {k}\\n')\n",
    "\n",
    "                for l in range(0, k):\n",
    "                    f.write('******************************************\\n')\n",
    "                    f.write(f'Cluster {l}\\n')\n",
    "                    f.write('******************************************\\n')\n",
    "                    f.write(df['tweet_text'][(df['y'] == l)].sample(5).to_string())\n",
    "\n",
    "                f.write(f'------------------------------------------------------------------------\\n')\n",
    "                f.write(f'------------------------------------------------------------------------\\n')\n",
    "                print(f'min_df = {i} ---- max_features = {j} ---- n_clusters = {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Useful snippets etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Memory usage https://stackoverflow.com/questions/16261240/releasing-memory-of-huge-numpy-array-in-ipython/16278056\n",
    "# import sys\n",
    "# def sizeof_fmt(num, suffix='B'):\n",
    "#     ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "#         if abs(num) < 1024.0:\n",
    "#             return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "#         num /= 1024.0\n",
    "#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "#                          key= lambda x: -x[1])[:10]:\n",
    "#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a number of tweets for each cluster\n",
    "# y = pd.Series(y_kmeans)\n",
    "# df_tweet_y = pd.DataFrame(en_tweets_df)\n",
    "# df_tweet_y['y'] = pd.Series(y)\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# print(f'------------------------------------------------------------------------')\n",
    "# print(f'------------------------------------------------------------------------')\n",
    "# # print(f'k = {k} ---- Number of FT cols = {num_of_FT_cols}')\n",
    "# print(f'Data shape = {X.shape}')\n",
    "\n",
    "# for i in range(0, k):\n",
    "#     print(f'****************************************** \\n Cluster {i}')\n",
    "#     print('******************************************')\n",
    "#     print(df_tweet_y['tweet_text'][(df_tweet_y['y'] == i)].sample(15).to_string())\n",
    "#     print('****************************************** \\n')\n",
    "\n",
    "# print(f'------------------------------------------------------------------------')\n",
    "# print(f'------------------------------------------------------------------------\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return 3 - 10 clusters for cummulative sets of 25 columns, the first 25 are the most common words\n",
    "# for k in range(3, 11):\n",
    "#     for num_of_FT_cols in range(25, 501, 25):\n",
    "#         # Create X ensuring only records that have at least one of the features are included\n",
    "#         X = en_tweets_df_with_features[cols[:num_of_FT_cols]]\n",
    "#         X['sum_of_FT_cols'] = X.sum(axis=1)\n",
    "#         X = X[cols[:num_of_FT_cols]][X['sum_of_FT_cols'] != 0]\n",
    "\n",
    "#         # Fit and predict model\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "#         kmeans.fit(X)\n",
    "#         y_kmeans = kmeans.predict(X)\n",
    "        \n",
    "#         # Print a number of tweets for each cluster\n",
    "#         y = pd.Series(y_kmeans)\n",
    "#         df_tweet_y = pd.DataFrame(en_tweets_df_with_features)\n",
    "#         df_tweet_y['y'] = pd.Series(y)\n",
    "\n",
    "#         pd.set_option('display.max_colwidth', None)\n",
    "        \n",
    "#         print(f'------------------------------------------------------------------------')\n",
    "#         print(f'------------------------------------------------------------------------')\n",
    "#         print(f'k = {k} ---- Number of FT cols = {num_of_FT_cols}')\n",
    "#         print(f'Data shape = {X.shape}')\n",
    "\n",
    "#         for i in range(0, k):\n",
    "#             print(f'****************************************** \\n Cluster {i}')\n",
    "#             print('******************************************')\n",
    "#             print(df_tweet_y['tweet_text'][(df_tweet_y['y'] == i) & (df_tweet_y['is_retweet'] == False)].sample(5).to_string())\n",
    "#             print('****************************************** \\n')\n",
    "            \n",
    "#         print(f'------------------------------------------------------------------------')\n",
    "#         print(f'------------------------------------------------------------------------\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the above diagrams 7 clusters with 225 looks like it could hold some insights\n",
    "# Create X ensuring only records that have at least one of the features are included\n",
    "# k = 6\n",
    "# X = en_tweets_df[cols]\n",
    "# # X['sum_of_FT_cols'] = X.sum(axis=1)\n",
    "# # X = X[cols][X['sum_of_FT_cols'] != 0]\n",
    "\n",
    "# # Fit and predict model\n",
    "# kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "# kmeans.fit(X)\n",
    "# y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify the optimal number of clusters (k), also considering the number of feature columns. This should assist in review of the cluster samples output.\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# K = range(3, 11)\n",
    "# for num_of_FT_cols in range(25, 251, 25):\n",
    "#     distortions = []\n",
    "#     for k in K:\n",
    "#         # Create X ensuring only records that have at least one of the features are included\n",
    "#         X = en_tweets_df_with_features[cols[:num_of_FT_cols]]\n",
    "#         X['sum_of_FT_cols'] = X.sum(axis=1)\n",
    "#         X = X[cols[:num_of_FT_cols]][X['sum_of_FT_cols'] != 0]\n",
    "\n",
    "#         X_scaled = preprocessing.scale(X)\n",
    "\n",
    "#         # Fit and predict model\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=8)\n",
    "#         kmeans.fit(X_scaled)\n",
    "#         distortions.append(kmeans.inertia_)\n",
    "        \n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(K, distortions, 'bx-')\n",
    "#     plt.xlabel('k')\n",
    "#     plt.ylabel('Distortion')\n",
    "#     plt.title(f'The Elbow Method showing the optimal k with {num_of_FT_cols} feature columns')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run PCA on the data and reduce the dimensions in pca_num_components dimensions\n",
    "# reduced_data = PCA(n_components=2).fit_transform(X)\n",
    "# results = pd.DataFrame(reduced_data,columns=['pca1','pca2'])\n",
    "\n",
    "# plt.figure(figsize=(40,30))\n",
    "# sns.scatterplot(x=\"pca1\", y=\"pca2\", hue=y_kmeans, data=results)\n",
    "# plt.title('K-means Clustering')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
